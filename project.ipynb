{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vwZTW7VkdUh4"
   },
   "outputs": [],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DJzI2bkqdhGg"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lfPL2ZZAf2ID"
   },
   "outputs": [],
   "source": [
    "!mkdir ~/.kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZHj23dOxgDrr"
   },
   "outputs": [],
   "source": [
    "#! cp kaggle.json  ~/.kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UMdTjxChgU4i"
   },
   "outputs": [],
   "source": [
    "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\"\"\"your kaggle json file\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvrGyjBrgl0R"
   },
   "outputs": [],
   "source": [
    "! chmod 600 ~/.kaggle/\"\"\"your kaggle json file\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q-uvkwxMiFVy"
   },
   "outputs": [],
   "source": [
    "!kaggle datasets download pranavraikokte/pneumoniadataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ptFtnvCiiiD"
   },
   "outputs": [],
   "source": [
    "! unzip pneumoniadataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_6_BijGUKN4Z"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EufhgoUdKQOa"
   },
   "outputs": [],
   "source": [
    "# Set up the data generators for loading and preprocessing the images\n",
    "data_dir = '/content/Pnuemonia'\n",
    "batch_size = 32\n",
    "img_size = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YxGXsF80KUZR"
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8qWLjzzuKUXh"
   },
   "outputs": [],
   "source": [
    "train_data = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_6lx5mCjKXzr"
   },
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.VGG16(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(img_size[0], img_size[1], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQ-gnRiZKcD2"
   },
   "outputs": [],
   "source": [
    "train_features = base_model.predict(train_data, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TwavKXHwKfJg"
   },
   "outputs": [],
   "source": [
    "train_features = np.reshape(train_features, (train_features.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H9MO0WxaKmAq"
   },
   "outputs": [],
   "source": [
    "svm = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "svm.fit(train_features, train_data.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "57F26wdyKshb"
   },
   "outputs": [],
   "source": [
    "train_preds = svm.predict(train_features)\n",
    "print(classification_report(train_data.labels, train_preds))\n",
    "print(confusion_matrix(train_data.labels, train_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w3Itz3wlLHA6"
   },
   "outputs": [],
   "source": [
    "selector = SelectKBest(chi2, k=1000)\n",
    "train_features = selector.fit_transform(train_features, train_data.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AyAlGCYcLJro"
   },
   "outputs": [],
   "source": [
    "svm = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "svm.fit(train_features, train_data.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "91tBTEUrLSP8"
   },
   "outputs": [],
   "source": [
    "train_preds = svm.predict(train_features)\n",
    "print(classification_report(train_data.labels, train_preds))\n",
    "print(confusion_matrix(train_data.labels, train_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CSeN7slnLUVb"
   },
   "outputs": [],
   "source": [
    "image_path=\"/content/Pnuemonia/Positive/patient00085_study1_view1_frontal.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PWxYl3DHLZV4"
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(224, 224))\n",
    "    x = img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = tf.keras.applications.vgg16.preprocess_input(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BGqFJBmcPPp_"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_features(image_path):\n",
    "    x = preprocess_image(image_path)\n",
    "    features = base_model.predict(x)\n",
    "    features = np.reshape(features, (features.shape[0], -1))\n",
    "    return features\n",
    "new_features=extract_features(image_path)\n",
    "new_features=selector.transform(new_features)\n",
    "prediction=svm.predict(new_features)\n",
    "\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VBgNLBQP4PNd"
   },
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "dump(svm, 'svm.joblib')\n",
    "dump(selector, 'selector.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_d2wq8XT7N8G"
   },
   "outputs": [],
   "source": [
    "loadedSvm=load('svm.joblib')\n",
    "loadedSelector=load('selector.joblib')\n",
    "# Load the image and preprocess it\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(224, 224))\n",
    "    x = img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = tf.keras.applications.vgg16.preprocess_input(x)\n",
    "    return x\n",
    "\n",
    "def extract_features(image_path):\n",
    "    x = preprocess_image(image_path)\n",
    "    features = base_model.predict(x)\n",
    "    features = np.reshape(features, (features.shape[0], -1))\n",
    "    return features\n",
    "\n",
    "new_features = extract_features(image_path)\n",
    "new_features = loadedSelector.transform(new_features)\n",
    "prediction = loadedSvm.predict(new_features)\n",
    "\n",
    "print(prediction)\n",
    "\n",
    "if prediction==0:\n",
    "  print(\"The person has pneumonia\")\n",
    "\n",
    "else:\n",
    "  print(\"The person is healthy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FtYnLVTOPPqI"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rYcQIdRjPPqJ"
   },
   "outputs": [],
   "source": [
    "# steps for implementing cnn\n",
    "    #Choose a dataset and import libraries\n",
    "    #Prepare a dataset for training ie assigning paths and creating categories(labels), resizing our images\n",
    "\n",
    "    #Create training data\n",
    "    # Shuffle the dataset\n",
    "    #assign labels and features\n",
    "    # Normalising X and converting labels into categorical data\n",
    "    #Split X and Y for use in CNN\n",
    "    #Define, compile and train the CNN Model\n",
    "    # Accuracy and score of model\n",
    "    # we are going to utilize the kaggle dataset that is made up of 1000 images 500 for pneumonia and 500 for normal chest x_rays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50KKwH0NPPqK"
   },
   "outputs": [],
   "source": [
    "#The libraries\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "import numpy as np\n",
    "# we use glob to retrieve files in folders and sub folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQwR_MGVPPqL"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "import imghdr\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3i06XvFBPPqM"
   },
   "outputs": [],
   "source": [
    "# To see available devices\n",
    "gpus=tf.config.experimental.list_physical_devices('CPU')\n",
    "gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZxQvWZhYPPqM"
   },
   "outputs": [],
   "source": [
    "# limiting tensor from using all gpu space\n",
    "gpus=tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2YW0yek9PPqO"
   },
   "outputs": [],
   "source": [
    "# loading images\n",
    "# imghdr allows us to check extensions\n",
    "\n",
    "data_dir='/content/Pnuemonia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gNmG55IgPPqO"
   },
   "outputs": [],
   "source": [
    "# checking subfolders\n",
    "os.listdir(data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7hpZynRDPPqP"
   },
   "outputs": [],
   "source": [
    "# checking images in negative folder\n",
    "os.listdir(os.path.join(data_dir, 'Negative'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yf2IRRarPPqQ"
   },
   "outputs": [],
   "source": [
    "# accepted extensions\n",
    "image_exts=['jpeg', 'jpg', 'bmp', 'png']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TnP0HmEtPPqQ"
   },
   "outputs": [],
   "source": [
    "# running extensions\n",
    "image_exts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iE9jV1h_PPqR"
   },
   "outputs": [],
   "source": [
    "# image class print negative or positive\n",
    "for image_class in os.listdir(data_dir):\n",
    "    print(image_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9PHrJmCPPqS"
   },
   "outputs": [],
   "source": [
    "# for loop 2 below prints every single image in that directory\n",
    "for image_class in os.listdir(data_dir):\n",
    "    for image in os.listdir(os.path.join(data_dir, image_class)):\n",
    "        print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-tSeGWezPPqT"
   },
   "outputs": [],
   "source": [
    "# checking if open cv allows us to open all images\n",
    "img=cv2.imread(os.path.join('/content/Pnuemonia', 'Negative', 'patient00012_study3_view1_frontal.jpg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ztDAACgPPPqU"
   },
   "outputs": [],
   "source": [
    "# checking the properties of our image\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bIqHe5xhPPqV"
   },
   "outputs": [],
   "source": [
    "# opening our image\n",
    "plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XylOXLHlPPqW"
   },
   "outputs": [],
   "source": [
    "# changing the color to be normal\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "# to get rid of <matplotlib.image.AxesImage at 0x7f6fff0f1570>\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ab12hZPTPPqW"
   },
   "outputs": [],
   "source": [
    "# removing corrupted images\n",
    "for image_class in os.listdir(data_dir):\n",
    "    for image in os.listdir(os.path.join(data_dir, image_class)):\n",
    "        image_path=os.path.join(data_dir, image_class, image)\n",
    "        try:\n",
    "            img=cv2.imread(image_path)\n",
    "            tip=imghdr.what(image_path)\n",
    "            if tip not in image_exts:\n",
    "                print('Image not in extension list {}'.format(image_path))\n",
    "                os.remove(image_path)\n",
    "        except Exception as e:\n",
    "            print('Issue with image {}'.format(image_path))\n",
    "            # os.remove(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fiZVSybKPPqX"
   },
   "outputs": [],
   "source": [
    "# to show what keras can do\n",
    "tf.keras.utils.image_dataset_from_directory??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NuRsU8ncPPqX"
   },
   "outputs": [],
   "source": [
    "# now loading our data\n",
    "# adds classes and labels and some preprocessing\n",
    "#building our data pipeline\n",
    "data=tf.keras.utils.image_dataset_from_directory('/content/Pnuemonia')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s5C3q3W2PPqX"
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mEYCM9bIPPqY"
   },
   "outputs": [],
   "source": [
    "# in order to check the data we first convert it into a numpy iterator first\n",
    "#looping through our data pipeline\n",
    "data_iterator=data.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "seJYNCcdPPqY"
   },
   "outputs": [],
   "source": [
    "data_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDVtX2O1PPqY"
   },
   "outputs": [],
   "source": [
    "# to run the .next line\n",
    "#accessing our data pipeline\n",
    "batch=data_iterator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "acDW683sPPqY"
   },
   "outputs": [],
   "source": [
    "\n",
    "# looking through our data pipeline\n",
    "batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yqahYxHEPPqZ"
   },
   "outputs": [],
   "source": [
    "# looking at the shape of the batch\n",
    "# images represented as numpy arrays\n",
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GJtlwgfQPPqZ"
   },
   "outputs": [],
   "source": [
    "#labels\n",
    "batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sYtkiiHuPPqZ"
   },
   "outputs": [],
   "source": [
    "len(batch)\n",
    "# it is two because the first part is the images and the second is the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hz5yqQQmPPqZ"
   },
   "outputs": [],
   "source": [
    "# to check which class ie either 0 or 1 is attached to which class ie Negative or positive\n",
    "fig, ax=plt.subplots(ncols=4, figsize=(20, 20))\n",
    "for idx, img in enumerate(batch[0][:4]):\n",
    "    ax[idx].imshow(img.astype(int))\n",
    "    ax[idx].title.set_text(batch[1][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RsHd2YtmPPqa"
   },
   "outputs": [],
   "source": [
    "# preprocessing the data\n",
    "# scaling the data to be between 0 and 1 not between 0 and 255\n",
    "# done when we prefetch data ie transformation in pipeline\n",
    "data=data.map(lambda  x,y:(x/255, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NcU6042PPPqa"
   },
   "outputs": [],
   "source": [
    "# taking a looking at our data batch, we can see that it is scaled in the pipeline\n",
    "scaled_iterator=data.as_numpy_iterator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JBvQ1xEKPPqa"
   },
   "outputs": [],
   "source": [
    "batch=scaled_iterator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uTGfnYkOPPqa"
   },
   "outputs": [],
   "source": [
    "batch[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o7vPvLFbPPqa"
   },
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(ncols=4, figsize=(20, 20))\n",
    "for idx, img in enumerate(batch[0][:4]):\n",
    "    ax[idx].imshow(img)\n",
    "    ax[idx].title.set_text(batch[1][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VNuut_D0PPqb"
   },
   "outputs": [],
   "source": [
    "# checking the length of data\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n5jJYRBnPPqf"
   },
   "outputs": [],
   "source": [
    "# Splitting our data\n",
    "train_size=int(len(data)*.7)\n",
    "val_size=int(len(data)*.2)\n",
    "test_size=int(len(data)*.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y0wBR2t5PPqf"
   },
   "outputs": [],
   "source": [
    "# size of training data\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6w9ks_-tPPqg"
   },
   "outputs": [],
   "source": [
    "# size of validation data\n",
    "val_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OhKquRRePPqg"
   },
   "outputs": [],
   "source": [
    "# size of test data\n",
    "test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o__98nKdPPqg"
   },
   "outputs": [],
   "source": [
    "# checking if all the length of the data adds up\n",
    "# training for training, validation for testing while training and testing for testing after completion\n",
    "train_size+val_size+test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BlXpQvLxPPqg"
   },
   "outputs": [],
   "source": [
    "# in order to use this data we use the take and skip variable inside the tensorflow dataset pipeline\n",
    "train=data.take(train_size)\n",
    "val=data.skip(train_size).take(val_size)\n",
    "test=data.skip(train_size+val_size).take(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a71M46lqPPqh"
   },
   "outputs": [],
   "source": [
    "# Creating our deep learning model\n",
    "# Build a deep learning model\n",
    "model=Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iONvn_H8PPqh"
   },
   "outputs": [],
   "source": [
    "# adding our layers\n",
    "# EG in the first convolution layer we have got 16 filters, which have 3 pixels by 3 pixels and we have a stride of 1\n",
    "model.add(Conv2D(16, (3, 3), 1, activation='relu', input_shape=(256, 256, 3)))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), 1, activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "model.add(Conv2D(16, (3, 3), 1, activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WzUti65UPPqh"
   },
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "model.compile('adam', loss=tf.losses.BinaryCrossentropy(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MwsdOzp_PPqi"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "llETXjSDPPqk"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "# we create a log directory\n",
    "logdir='/home/mylaptop/Downloads/archive (5)/Logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WOzl73NaPPql"
   },
   "outputs": [],
   "source": [
    "# useful if you need to save your model at a particular checkpoint or if you want to do a specific logging\n",
    "tensorboard_callback=tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q2-1vYAv8pRE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lbxrd0RSPPql"
   },
   "outputs": [],
   "source": [
    "# fitting our model\n",
    "#epochs=how long we are going to train for ie one run over the training data\n",
    "hist=model.fit(train, epochs=20, validation_data=val, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JbIwGj2hPPql"
   },
   "outputs": [],
   "source": [
    "hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LT87SpRUPPqn"
   },
   "outputs": [],
   "source": [
    "# plot perfomance\n",
    "# visualizing loss\n",
    "plt.plot(hist.history['loss'], color='teal', label='loss')\n",
    "plt.plot(hist.history['val_loss'], color='orange', label='val_loss')\n",
    "plt.suptitle('Loss', fontsize=20)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()\n",
    "# check regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZdL39VQdPPqo"
   },
   "outputs": [],
   "source": [
    "# visualizing accuracy\n",
    "fig=plt.figure()\n",
    "plt.plot(hist.history['accuracy'], color='teal', label='accuracy')\n",
    "plt.plot(hist.history['val_accuracy'], color='orange', label='val_accuracy')\n",
    "fig.suptitle('Accuracy', fontsize=20)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kVdQPbZ9PPqp"
   },
   "outputs": [],
   "source": [
    "#Evaluating perfomance\n",
    "# establishing instances of precision, recall and binary accuracy\n",
    "pre=Precision()\n",
    "re=Recall()\n",
    "acc=BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FskTZh2iPPqp"
   },
   "outputs": [],
   "source": [
    "# testing the imports in our data\n",
    "for batch in test.as_numpy_iterator():\n",
    "    X, y=batch\n",
    "    yhat=model.predict(X)\n",
    "    pre.update_state(y, yhat)\n",
    "    re.update_state(y, yhat)\n",
    "    acc.update_state(y, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3AyWsx3tPPqq"
   },
   "outputs": [],
   "source": [
    "# printing the results of prediction, recall and accuracy\n",
    "print(f'Precision:{pre.result().numpy()}, Recall:{re.result().numpy()}, Accuracy:{acc.result().numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0fwk6ApjPPqq"
   },
   "outputs": [],
   "source": [
    "# Testing phase\n",
    "# pneumonic test\n",
    "#NormalHealthy\n",
    "#pneumoniaPositive\n",
    "img=cv2.imread('/content/Pnuemonia/Negative/patient00063_study3_view1_frontal.jpg')\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aj1tByRPPPqr"
   },
   "outputs": [],
   "source": [
    "# editing the image to fit our details.\n",
    "resize=tf.image.resize(img, (256, 256))\n",
    "plt.imshow(cv2.cvtColor(resize.numpy(), cv2.COLOR_BGR2RGB).astype(int))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L6_KU44PPPqr"
   },
   "outputs": [],
   "source": [
    "\n",
    "# encapsulating the image since our neural network expects many images but we are only passing one. and then showing its shape\n",
    "np.expand_dims(resize, 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8iPsXP7yPPqr"
   },
   "outputs": [],
   "source": [
    "# PREDICTING IF IT IS PNEUMONIC OR NOT\n",
    "yhat=model.predict(np.expand_dims(resize/255, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1hjwzbp6PPqs"
   },
   "outputs": [],
   "source": [
    "#prediction\n",
    "yhat\n",
    "#NB pneumonic is 1\n",
    "#Normal is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IbVx5Y1VPPqs"
   },
   "outputs": [],
   "source": [
    "if yhat>0.5:\n",
    "    print(\"The person has pneumonia\")\n",
    "else:\n",
    "    print(\"The person is normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RQ17jlvNkmRL"
   },
   "outputs": [],
   "source": [
    "#model_feature_extractor = Sequential(model.layers[:-1])\n",
    "#train_features = model_feature_extractor.predict(train)\n",
    "#val_features = model_feature_extractor.predict(val)\n",
    "#test_features = model_feature_extractor.predict(test)\n",
    "\n",
    "train_data = ImageDataGenerator().flow_from_directory('/content/Pnuemonia/', target_size=(256, 256), batch_size=batch_size, class_mode=None)\n",
    "train_labels = train_data.classes\n",
    "\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# create an LDA object\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "# fit the LDA model on the training data and transform the data\n",
    "train_features = lda.fit_transform(train_features, train_labels)\n",
    "\n",
    "# define a new model\n",
    "model_lda = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(train_features.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# compile the model\n",
    "model_lda.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', Precision(), Recall()])\n",
    "\n",
    "model_lda.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "igW6xaclkpyX"
   },
   "outputs": [],
   "source": [
    "hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q5jFIvlUkzUD"
   },
   "outputs": [],
   "source": [
    "# plot perfomance\n",
    "# visualizing loss\n",
    "plt.plot(hist.history['loss'], color='teal', label='loss')\n",
    "plt.plot(hist.history['val_loss'], color='orange', label='val_loss')\n",
    "plt.suptitle('Loss', fontsize=20)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()\n",
    "# check regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uJNYZMSRk59X"
   },
   "outputs": [],
   "source": [
    "# visualizing accuracy\n",
    "fig=plt.figure()\n",
    "plt.plot(hist.history['accuracy'], color='teal', label='accuracy')\n",
    "plt.plot(hist.history['val_accuracy'], color='orange', label='val_accuracy')\n",
    "fig.suptitle('Accuracy', fontsize=20)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XWGar5p5k7WB"
   },
   "outputs": [],
   "source": [
    "#Evaluating perfomance\n",
    "# establishing instances of precision, recall and binary accuracy\n",
    "pre=Precision()\n",
    "re=Recall()\n",
    "acc=BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zAIOTzo0lYGx"
   },
   "outputs": [],
   "source": [
    "# testing the imports in our data\n",
    "for batch in test.as_numpy_iterator():\n",
    "    X, y=batch\n",
    "    yhat=model.predict(X)\n",
    "    pre.update_state(y, yhat)\n",
    "    re.update_state(y, yhat)\n",
    "    acc.update_state(y, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tTDwJWDklwEm"
   },
   "outputs": [],
   "source": [
    "# printing the results of prediction, recall and accuracy\n",
    "print(f'Precision:{pre.result().numpy()}, Recall:{re.result().numpy()}, Accuracy:{acc.result().numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0bFHpVakl06q"
   },
   "outputs": [],
   "source": [
    "# Testing phase\n",
    "# pneumonic test\n",
    "#NormalHealthy\n",
    "#pneumoniaPositive\n",
    "img=cv2.imread('/content/Pnuemonia/Negative/patient00063_study3_view1_frontal.jpg')\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-AiDWNJSmGMj"
   },
   "outputs": [],
   "source": [
    "# editing the image to fit our details.\n",
    "resize=tf.image.resize(img, (256, 256))\n",
    "plt.imshow(cv2.cvtColor(resize.numpy(), cv2.COLOR_BGR2RGB).astype(int))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YxD6ll0omKDL"
   },
   "outputs": [],
   "source": [
    "# encapsulating the image since our neural network expects many images but we are only passing one. and then showing its shape\n",
    "np.expand_dims(resize, 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gkABT4f0mZBa"
   },
   "outputs": [],
   "source": [
    "# PREDICTING IF IT IS PNEUMONIC OR NOT\n",
    "yhat=model.predict(np.expand_dims(resize/255, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BoM-C5GqmgkG"
   },
   "outputs": [],
   "source": [
    "#prediction\n",
    "yhat\n",
    "#NB pneumonic is 1\n",
    "#Normal is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sdob1EbNmv0R"
   },
   "outputs": [],
   "source": [
    "if yhat>0.5:\n",
    "    print(\"The person has pneumonia\")\n",
    "else:\n",
    "    print(\"The person is normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b-SSBuH7PPqs"
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "# This will create a file name pneumoniaPredictionModel.h5 in the models folder\n",
    "model.save(os.path.join('/content/Pnuemonia/models', 'pneumoniaPredictionModel.h5'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4yQXzTrKjwP"
   },
   "outputs": [],
   "source": [
    "#This is for reloading the model\n",
    "new_model=load_model(os.path.join('/content/Pnuemonia/models', 'pneumoniaPredictionModel.h5'))\n",
    "# read the model\n",
    "# passing data to the model and predicting\n",
    "yhat_new=new_model.predict(np.expand_dims(resize/255, 0))\n",
    "if yhat_new>0.5:\n",
    "  print(\"The person has pneumonia\")\n",
    "else:\n",
    "  print(\"The person is healthy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o84pY-rkNrfC"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "labels=['Support Vector Machine', 'Convolutional Neural Network']\n",
    "accuracy=[100, 92.31]\n",
    "precision=[100, 90.91]\n",
    "recall=[100, 93.98]\n",
    "\n",
    "fig, ax=plt.subplots()\n",
    "\n",
    "bar_width=0.15\n",
    "\n",
    "x_accuracy=np.arange(len(labels))\n",
    "\n",
    "x_recall=x_accuracy+bar_width\n",
    "x_precision=x_accuracy+2*bar_width\n",
    "\n",
    "accuracy_bars=ax.bar(x_accuracy-bar_width/2, accuracy, bar_width, label='ACCURACY')\n",
    "recall_bars=ax.bar(x_recall-bar_width/2, accuracy, bar_width, label='RECALL')\n",
    "precision_bars=ax.bar(x_precision-bar_width/2, accuracy, bar_width, label='PRECISION')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "ax.set_ylim([0, 120])\n",
    "\n",
    "ax.set_xticks(x_precision-bar_width/2)\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "ax.set_title(\"PERFOMANCE METRICS BEFORE FEATURE SELECTION\")\n",
    "ax.set_xlabel(\"ALGORITHM USED\")\n",
    "ax.set_ylabel(\"PERCENTAGE ACHIEVED\")\n",
    "\n",
    "ax.set_xlim([x_accuracy[0]-bar_width, x_precision[-1] + bar_width])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5227wWyW77x"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3reY0fROW_gB"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "labels=['Positive SVM', 'Negative SVM', 'CNN']\n",
    "accuracy=[97, 97, 90.10]\n",
    "precision=[98, 96, 91.76]\n",
    "recall=[96, 97, 86.87]\n",
    "\n",
    "fig, ax=plt.subplots()\n",
    "\n",
    "bar_width=0.15\n",
    "\n",
    "x_accuracy=np.arange(len(labels))\n",
    "\n",
    "x_recall=x_accuracy+bar_width\n",
    "x_precision=x_accuracy+2*bar_width\n",
    "\n",
    "accuracy_bars=ax.bar(x_accuracy-bar_width/2, accuracy, bar_width, label='ACCURACY')\n",
    "recall_bars=ax.bar(x_recall-bar_width/2, accuracy, bar_width, label='RECALL')\n",
    "precision_bars=ax.bar(x_precision-bar_width/2, accuracy, bar_width, label='PRECISION')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "ax.set_ylim([0, 120])\n",
    "\n",
    "ax.set_xticks(x_precision-bar_width/2)\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "ax.set_title(\"PERFOMANCE METRICS AFTER FEATURE SELECTION\")\n",
    "ax.set_xlabel(\"ALGORITHM USED\")\n",
    "ax.set_ylabel(\"PERCENTAGE ACHIEVED\")\n",
    "\n",
    "ax.set_xlim([x_accuracy[0]-bar_width, x_precision[-1] + bar_width])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "a5a06c0fd267306085d6e2feacabab4d8b50b6ec923c4e571754f5723fc2401b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
